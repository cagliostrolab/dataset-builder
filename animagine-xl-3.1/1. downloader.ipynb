{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c777282-9f36-446e-80b9-4efaa334c798",
   "metadata": {},
   "source": [
    "# Scraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "929b2dfb-d523-46b6-bec1-d31de3f927c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gallery-dl tqdm -U\n",
    "!apt update -yqq\n",
    "!apt install aria2 -yqq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d22457f-1cbb-4294-b8de-d35071d9e695",
   "metadata": {},
   "source": [
    "## Grab JSON files by tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75a2a0-79db-4d6e-bdb0-7bee31d66a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.parse\n",
    "import threading\n",
    "from queue import Queue\n",
    "from tqdm import tqdm\n",
    "\n",
    "def scrape_tags_images(tags_name, queue, pbar):\n",
    "    encoded_tags_name = urllib.parse.quote_plus(tags_name)\n",
    "    url = f\"https://danbooru.donmai.us/posts?tags={encoded_tags_name}+&z=5\"\n",
    "    dir_path = f\"/workspace/train_data_json/animagine-xl-3.1/{encoded_tags_name}\"\n",
    "\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)\n",
    "\n",
    "    command = f\"gallery-dl '{url}' -D '{dir_path}' --write-metadata --no-download --user-agent 'gdl/1.26.7' --username 'Isaacvincrad' --password 'u83VTMCYQRQdD1Q4xf3FFqDe'\"\n",
    "\n",
    "    os.system(command)\n",
    "    queue.task_done()\n",
    "    pbar.update(1)\n",
    "\n",
    "def worker(queue, pbar):\n",
    "    while True:\n",
    "        tags_name = queue.get()\n",
    "        if tags_name is None:\n",
    "            break  # Exit signal\n",
    "        scrape_tags_images(tags_name, queue, pbar)\n",
    "\n",
    "def main():\n",
    "    tags_names = []\n",
    "        \n",
    "    queue = Queue()\n",
    "    threads = []\n",
    "    num_worker_threads = 4\n",
    "\n",
    "    with tqdm(total=len(tags_names), desc=\"Scraping Images\") as pbar:\n",
    "        for _ in range(num_worker_threads):\n",
    "            t = threading.Thread(target=worker, args=(queue, pbar))\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "\n",
    "        for tags_name in tags_names:\n",
    "            queue.put(tags_name)\n",
    "\n",
    "        queue.join()\n",
    "\n",
    "        for _ in range(num_worker_threads):\n",
    "            queue.put(None)\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "    print(\"Scraping complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3fe59f-8cc2-479d-b618-23d64060ca58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import requests\n",
    "import threading\n",
    "import shutil\n",
    "import subprocess\n",
    "import dateutil.parser\n",
    "from queue import Queue\n",
    "from tqdm import tqdm\n",
    "\n",
    "total_files = 0\n",
    "downloaded_files = 0\n",
    "download_lock = threading.Lock()\n",
    "user_agent = 'gdl/1.25.8'\n",
    "\n",
    "meta_keywords_black_list = [\n",
    "    \"(medium)\", \"commentary\",\n",
    "    \"bad\", \"translat\", \"request\", \n",
    "    \"mismatch\", \"revision\", \"audio\",\n",
    "    \"video\", \n",
    "]\n",
    "\n",
    "special_tags = [\n",
    "    \"1girl\", \"2girls\", \"3girls\", \"4girls\", \"5girls\", \"6+girls\", \"multiple girls\",\n",
    "    \"1boy\", \"2boys\", \"3boys\", \"4boys\", \"5boys\", \"6+boys\", \"multiple boys\", \"male focus\",\n",
    "    \"1other\", \"2others\", \"3others\", \"4others\", \"5others\", \"6+others\", \"multiple others\", \"other focus\",\n",
    "]\n",
    "\n",
    "score_percentile_full = {\n",
    "    \"g\": {\n",
    "        5: 0,\n",
    "        10: 1,\n",
    "        15: 2,\n",
    "        20: 3,\n",
    "        25: 3,\n",
    "        30: 4,\n",
    "        35: 5,\n",
    "        40: 5,\n",
    "        45: 6,\n",
    "        50: 7,\n",
    "        55: 8,\n",
    "        60: 9,\n",
    "        65: 11,\n",
    "        70: 12,\n",
    "        75: 14,\n",
    "        80: 16,\n",
    "        85: 19,\n",
    "        90: 24,\n",
    "        95: 33,\n",
    "    },\n",
    "    \"s\": {\n",
    "        5: 0,\n",
    "        10: 1,\n",
    "        15: 2,\n",
    "        20: 3,\n",
    "        25: 4,\n",
    "        30: 5,\n",
    "        35: 6,\n",
    "        40: 7,\n",
    "        45: 8,\n",
    "        50: 9,\n",
    "        55: 11,\n",
    "        60: 12,\n",
    "        65: 15,\n",
    "        70: 17,\n",
    "        75: 20,\n",
    "        80: 25,\n",
    "        85: 31,\n",
    "        90: 41,\n",
    "        95: 62,\n",
    "    },\n",
    "    \"q\": {\n",
    "        5: 2,\n",
    "        10: 4,\n",
    "        15: 5,\n",
    "        20: 7,\n",
    "        25: 9,\n",
    "        30: 11,\n",
    "        35: 14,\n",
    "        40: 16,\n",
    "        45: 19,\n",
    "        50: 23,\n",
    "        55: 26,\n",
    "        60: 31,\n",
    "        65: 36,\n",
    "        70: 42,\n",
    "        75: 49,\n",
    "        80: 59,\n",
    "        85: 73,\n",
    "        90: 93,\n",
    "        95: 134,\n",
    "    },\n",
    "    \"e\": {\n",
    "        5: 2,\n",
    "        10: 4,\n",
    "        15: 7,\n",
    "        20: 10,\n",
    "        25: 13,\n",
    "        30: 17,\n",
    "        35: 20,\n",
    "        40: 25,\n",
    "        45: 29,\n",
    "        50: 35,\n",
    "        55: 41,\n",
    "        60: 48,\n",
    "        65: 56,\n",
    "        70: 66,\n",
    "        75: 78,\n",
    "        80: 94,\n",
    "        85: 115,\n",
    "        90: 148,\n",
    "        95: 211,\n",
    "    },\n",
    "}\n",
    "\n",
    "def aria2_download(dir, filename, url):\n",
    "\n",
    "    aria2_config = {\n",
    "        \"console-log-level\"         : \"error\",\n",
    "        \"summary-interval\"          : 10,\n",
    "        \"continue\"                  : True,\n",
    "        \"max-connection-per-server\" : 16,\n",
    "        \"min-split-size\"            : \"1M\",\n",
    "        \"split\"                     : 16,\n",
    "        \"dir\"                       : str(dir),\n",
    "        \"out\"                       : filename,\n",
    "        \"user-agent\"                : user_agent,\n",
    "        \"_url\"                      : url,\n",
    "    }\n",
    "    aria2_args = parse_args(aria2_config, aria=True)\n",
    "    subprocess.Popen([\"aria2c\", *aria2_args])\n",
    "    \n",
    "def parse_args(config, aria=False):\n",
    "    args = []\n",
    "\n",
    "    for k, v in config.items():\n",
    "        if k.startswith(\"_\"):\n",
    "            args.append(f\"{v}\")\n",
    "        elif isinstance(v, str) and v is not None:\n",
    "            if aria:\n",
    "                args.append(f\"--{k}={v}\")\n",
    "            else:\n",
    "                args.append(f\"--{k}='{v}'\")\n",
    "        elif isinstance(v, bool) and v:\n",
    "            args.append(f\"--{k}\")\n",
    "        elif isinstance(v, float) and not isinstance(v, bool):\n",
    "            args.append(f\"--{k}={v}\")\n",
    "        elif isinstance(v, int) and not isinstance(v, bool):\n",
    "            args.append(f\"--{k}={v}\")\n",
    "\n",
    "    return args\n",
    "    \n",
    "def download_image(folder, url, filename, extension):\n",
    "    global downloaded_files\n",
    "    full_path = f\"{folder}/{filename}.{extension}\"\n",
    "    if os.path.exists(full_path):\n",
    "        return False\n",
    "    try:\n",
    "        aria2_download(folder, f\"{filename}.{extension}\", url)\n",
    "        \n",
    "        if os.path.exists(full_path) and os.path.getsize(full_path) > 0:\n",
    "            with download_lock:\n",
    "                downloaded_files += 1\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "        return False\n",
    "        \n",
    "def write_metadata(folder, filename, tags):\n",
    "    with open(f\"{folder}/{filename}.txt\", \"w\") as f:\n",
    "        f.write(tags)\n",
    "\n",
    "def filter_blacklisted_tags(tags, blacklist):\n",
    "    tag_list = tags.split(\", \")\n",
    "    filtered_tags = [tag for tag in tag_list if not any(blacklist_keyword in tag for blacklist_keyword in blacklist)]\n",
    "    return \", \".join(filtered_tags)\n",
    "    \n",
    "def generate_tags(data):\n",
    "    def process_tags(tag_str):\n",
    "        processed_tags = []\n",
    "        for tag_name in tag_str.split(\" \"):\n",
    "            if len(tag_name) > 3:\n",
    "                tag_name = tag_name.replace(\"_\", \" \")\n",
    "            processed_tags.append(tag_name)\n",
    "        return \", \".join(processed_tags)\n",
    "\n",
    "    created_at = data.get(\"media_asset\", {}).get(\"created_at\", \"\")\n",
    "    year = 0\n",
    "    \n",
    "    try:\n",
    "        parsed_date = dateutil.parser.isoparse(created_at)\n",
    "        year = parsed_date.year\n",
    "    except:\n",
    "        pass\n",
    "        \n",
    "    if 2005 <= year <= 2010:\n",
    "        year_tag = \"oldest\"\n",
    "    elif year <= 2014:\n",
    "        year_tag = \"early\"\n",
    "    elif year <= 2017:\n",
    "        year_tag = \"mid\"\n",
    "    elif year <= 2020:\n",
    "        year_tag = \"recent\"\n",
    "    elif year <= 2024:\n",
    "        year_tag = \"newest\"\n",
    "    else:\n",
    "        year_tag = ''\n",
    "\n",
    "    rating = data.get(\"rating\")\n",
    "\n",
    "    score = data.get(\"score\")\n",
    "\n",
    "\n",
    "    tags_general = process_tags(data.get(\"tag_string_general\", \"\"))\n",
    "    tags_character = process_tags(data.get(\"tag_string_character\", \"\"))\n",
    "    tags_copyright = process_tags(data.get(\"tag_string_copyright\", \"\"))\n",
    "    tags_artist =  process_tags(data.get(\"tag_string_artist\", \"\"))\n",
    "    \n",
    "    tags_meta_raw = data.get(\"tag_string_meta\", \"\")\n",
    "    tags_meta = filter_blacklisted_tags(tags_meta_raw, meta_keywords_black_list)\n",
    "    tags_meta = process_tags(tags_meta)\n",
    "    \n",
    "    quality_tag = \"\"\n",
    "    percentile = score_percentile_full[rating]\n",
    "    \n",
    "    if score > percentile[95]:\n",
    "        quality_tag = \"masterpiece\"\n",
    "    elif score > percentile[85]:\n",
    "        quality_tag = \"best quality\"\n",
    "    elif score > percentile[75]:\n",
    "        quality_tag = \"great quality\"\n",
    "    elif score > percentile[50]:\n",
    "        quality_tag = \"good quality\"\n",
    "    elif score > percentile[25]:\n",
    "        quality_tag = \"normal quality\"\n",
    "    elif score > percentile[10]:\n",
    "        quality_tag = \"low quality\"\n",
    "    else:\n",
    "        quality_tag = \"worst quality\"\n",
    "    \n",
    "    if rating in \"e\":\n",
    "        nsfw_tags = \"explicit, nsfw\"\n",
    "    elif rating in \"q\":\n",
    "        nsfw_tags = \"nsfw\"\n",
    "    elif rating in \"s\":\n",
    "        nsfw_tags = \"sensitive\"\n",
    "    else:\n",
    "        nsfw_tags = \"safe\"\n",
    "\n",
    "    tags_general_list = tags_general.split(', ')\n",
    "\n",
    "    found_special_tags = [tag for tag in tags_general_list if tag in special_tags]\n",
    "\n",
    "    for tag in found_special_tags:\n",
    "        tags_general_list.remove(tag)\n",
    "\n",
    "    first_general_tag = ', '.join(found_special_tags)\n",
    "    rest_general_tags = ', '.join(tags_general_list)\n",
    "\n",
    "    tags_separator = \"|||\"\n",
    "    \n",
    "    pre_separator_tags = []\n",
    "    post_separator_tags = []\n",
    "\n",
    "    if first_general_tag:\n",
    "        pre_separator_tags.append(first_general_tag)\n",
    "    if tags_character:\n",
    "        pre_separator_tags.append(tags_character)\n",
    "    if tags_copyright:\n",
    "        pre_separator_tags.append(tags_copyright)\n",
    "    if tags_artist:\n",
    "        pre_separator_tags.append(tags_artist)\n",
    "\n",
    "    if nsfw_tags:\n",
    "        post_separator_tags.append(nsfw_tags)\n",
    "    if rest_general_tags:\n",
    "        post_separator_tags.append(rest_general_tags)\n",
    "    if year_tag:\n",
    "        post_separator_tags.append(year_tag)\n",
    "    if tags_meta:\n",
    "        post_separator_tags.append(tags_meta)\n",
    "    if quality_tag:\n",
    "        post_separator_tags.append(quality_tag)\n",
    "\n",
    "    pre_separator_str = ', '.join(pre_separator_tags)\n",
    "    post_separator_str = ', '.join(post_separator_tags)\n",
    "\n",
    "    caption = f\"{pre_separator_str}, {tags_separator} {post_separator_str}\"\n",
    "    \n",
    "    # print(caption)\n",
    "    # print()\n",
    "    return caption\n",
    "\n",
    "def process_file(json_folder, json_file):\n",
    "    with open(f\"{json_folder}/{json_file}\", \"r\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    extension = data.get(\"file_ext\")\n",
    "    rating_map = {'g': 'general', 's': 'sensitive', 'q': 'questionable', 'e': 'explicit'}\n",
    "    rating = rating_map.get(data.get(\"rating\"), \"\")\n",
    "    file_url = data.get(\"file_url\")\n",
    "\n",
    "    if extension not in [\"png\", \"jpg\", \"jpeg\", \"webp\", \"bmp\"]:\n",
    "        return\n",
    "\n",
    "    tags = generate_tags(data)\n",
    "    \n",
    "    if download_image(rating, file_url, json_file.split(\".\")[0], extension):\n",
    "        processed_folder = f\"{json_folder}_processed\"\n",
    "        os.makedirs(processed_folder, exist_ok=True)\n",
    "        shutil.move(f\"{json_folder}/{json_file}\", f\"{processed_folder}/{json_file}\")\n",
    "        \n",
    "    write_metadata(rating, json_file.split(\".\")[0], tags)\n",
    "\n",
    "def worker(queue, pbar):\n",
    "    while True:\n",
    "        item = queue.get()\n",
    "        if item is None:\n",
    "            # Signal to terminate this worker thread\n",
    "            queue.task_done()\n",
    "            break\n",
    "        json_file_path, json_folder = item  # Now safe to unpack\n",
    "        process_file(json_folder, os.path.basename(json_file_path))\n",
    "        queue.task_done()\n",
    "        pbar.update(1)  \n",
    "\n",
    "def main(json_folder):\n",
    "    global total_files\n",
    "    files_to_process = [(os.path.join(root, file), root) for root, dirs, files in os.walk(json_folder) for file in files if file.endswith('.json')]\n",
    "    total_files = len(files_to_process)\n",
    "\n",
    "    rating_folders = ['general', 'sensitive', 'questionable', 'explicit']\n",
    "    for folder in rating_folders:\n",
    "        if not os.path.exists(folder):\n",
    "            os.mkdir(folder)\n",
    "\n",
    "    queue = Queue()\n",
    "    threads = []\n",
    "    num_worker_threads = 8\n",
    "\n",
    "    with tqdm(total=total_files, desc=\"Processing Files\") as pbar:\n",
    "        for _ in range(num_worker_threads):\n",
    "            t = threading.Thread(target=worker, args=(queue, pbar))\n",
    "            t.start()\n",
    "            threads.append(t)\n",
    "\n",
    "        for file_path, folder in files_to_process:\n",
    "            queue.put((file_path, folder))\n",
    "\n",
    "        queue.join()\n",
    "\n",
    "        for _ in range(num_worker_threads):\n",
    "            queue.put(None)\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "\n",
    "    print(\"Scraping complete.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    project_path = \"/workspace/train_data/animagine-xl-3.1\"\n",
    "    os.makedirs(project_path, exist_ok=True)\n",
    "    os.chdir(project_path)\n",
    "    main(\"/workspace/train_data_json/animagine-xl-3.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d87b7f-ca97-485e-8ec5-d3f3344167c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def remove_ipynb_checkpoints(root_dir=''):\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir, topdown=False):  # Walk through the directory tree, from bottom to top\n",
    "        for dirname in dirnames:\n",
    "            if dirname == '__pycache__':\n",
    "                full_path = os.path.join(dirpath, dirname)\n",
    "                shutil.rmtree(full_path)  # Remove the directory and all its contents\n",
    "                print(f\"Removed: {full_path}\")\n",
    "\n",
    "# Call the function to start the removal process\n",
    "remove_ipynb_checkpoints()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
